{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Maslow's_Hierarchy_of_Needs.svg.png\" height = 600px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import urllib.parse # combine URL components into URL string\n",
    "import wikipediaapi # query wikipedia through api\n",
    "\n",
    "from pytrends.request import TrendReq # Google Trends API\n",
    "\n",
    "from statsmodels.tsa.seasonal import STL # seasonal decompositions\n",
    "import statsmodels.tsa.stattools as smt\n",
    "\n",
    "\n",
    "import pickle #  to serialize and deserialize objects in Python\n",
    "from requests.exceptions import RequestException\n",
    "import requests\n",
    "from json.decoder import JSONDecodeError\n",
    "from scipy import signal\n",
    "import warnings\n",
    "from urllib import request\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import pytrends\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import wikipediaapi\n",
    "import pprint as pp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a dataframe of pageviews per topic from a .txt using Wikipedia API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe(name_file):\n",
    "    \"\"\"\n",
    "    Creates a dataframe from a text file\n",
    "    param: name_file: name of the text file\n",
    "    return: dataframe with the text file\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(name_file, delimiter=\"\\t\", header=None, names=['Topics'])\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    return df\n",
    "\n",
    "# Take only starting from the second word in each row\n",
    "def remove_space(df):\n",
    "    return df['Topics'].apply(lambda x: x.strip().replace(' ', '_'))\n",
    "\n",
    "# Parse the topics into the URL format\n",
    "def parse_topics_into_df(df, lan, start_time, end_time):\n",
    "    # change the spaces to underscores\n",
    "    df['url'] = np.zeros(len(df))\n",
    "    for index, row in df.iterrows():\n",
    "        topic_value = row['Topics']\n",
    "        df.loc[index, 'url'] = f'https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/{lan}.wikipedia.org/all-access/all-agents/{topic_value}/monthly/{start_time}/{end_time}'\n",
    "\n",
    "    return df\n",
    "\n",
    "# Create a new dataframe with timestamp from starting date to ending date\n",
    "def create_dataframe_timestamp(starting_date, ending_date):\n",
    "    df_timestamp = pd.DataFrame()\n",
    "    df_timestamp['Timestamp'] = pd.date_range(start=starting_date, end=ending_date, freq='MS')\n",
    "    return df_timestamp\n",
    "\n",
    "# Define a function to fetch data from the URL and handle errors\n",
    "def fetch_and_parse_url(url):\n",
    "    try:\n",
    "        request.urlopen(url).read()\n",
    "        return True\n",
    "    except request.HTTPError as e:\n",
    "        if e.code == 404:\n",
    "            return False  # or any other value or action you prefer for 404 errors\n",
    "        else:\n",
    "            return False  # or handle other HTTP errors as needed\n",
    "    except Exception as e:\n",
    "        \n",
    "        return False  # or handle other exceptions as needed\n",
    "\n",
    "def get_pageviews_wiki(url):\n",
    "    \"\"\"\n",
    "    Gets the weekly pageviews for one Wikipedia page in one language in the desired period\n",
    "    param: url: url of the Wikipedia page\n",
    "    param: start_date: beginning of the desired period \n",
    "    param: end_date: end of the desired period \n",
    "    return: dataframe column with the monthly pageviews\n",
    "    \"\"\"\n",
    "    html = request.urlopen(url).read()\n",
    "    soup = BeautifulSoup(html,'html.parser')\n",
    "    site_json=json.loads(soup.text)\n",
    "    df=pd.DataFrame(site_json['items'])\n",
    "    df=df['views']\n",
    "    return df\n",
    "\n",
    "def get_mean_pageviews(df):\n",
    "    '''\n",
    "    'it': 'Italian','cs': 'Czech','ro': 'Romanian','sv': 'Swedish','fi': 'Finnish','da': 'Danish'\n",
    "    '''\n",
    "    # Check if all values in each column are equal to 0\n",
    "    df_time = df['Timestamp']\n",
    "    df = df.iloc[:, 1:]\n",
    "    all_zero_columns = (df == 0).all()\n",
    "\n",
    "    # Find row-wise mean\n",
    "    row_means = df[df != 0].mean(axis=1).fillna(0).round().astype(int)\n",
    "\n",
    "    # Replace columns with all zeros by row-wise means\n",
    "    for column in all_zero_columns[all_zero_columns].index:\n",
    "        df[column] = row_means\n",
    "    df.insert(0, 'Timestamp', df_time)\n",
    "    return df\n",
    "\n",
    "def scrape_pageviews(df):\n",
    "    pageview = pd.DataFrame()\n",
    "    pageview['Timestamp'] = create_dataframe_timestamp('2019-01-01', '2020-07-31')['Timestamp']\n",
    "\n",
    "    # Loop through the rows of the DataFrame and append the results of the function to the DataFrame\n",
    "    for index, row in df.iterrows():\n",
    "        url = row['url']\n",
    "        if fetch_and_parse_url(url):\n",
    "            pageview_solo = pd.DataFrame()\n",
    "            pageview_solo[row['Topics']] = get_pageviews_wiki(url)\n",
    "            pageview = pd.concat([pageview, pageview_solo], axis=1)\n",
    "\n",
    "    return pageview\n",
    "\n",
    "def scrape_pageviews_v2(df):\n",
    "    pageview = pd.DataFrame()\n",
    "    pageview['Timestamp'] = create_dataframe_timestamp('2019-01-01', '2020-07-31')['Timestamp']\n",
    "\n",
    "    # Loop through the rows of the DataFrame and append the results of the function to the DataFrame\n",
    "    for index, row in df.iterrows():\n",
    "        url = row['url']\n",
    "        if fetch_and_parse_url(url):\n",
    "            pageview_solo = pd.DataFrame()\n",
    "            pageview_solo[row['Topics']] = get_pageviews_wiki(url)\n",
    "            pageview = pd.concat([pageview, pageview_solo], axis=1)\n",
    "        else :\n",
    "            pageview_solo = pd.DataFrame()\n",
    "            pageview_solo[row['Topics']] = np.zeros(len(pageview))\n",
    "            pageview = pd.concat([pageview, pageview_solo], axis=1)\n",
    "\n",
    "    return pageview\n",
    "\n",
    "def create_dataframe_pageviews(name_file, lan, start_time = '20190101', end_time = '20200731'):\n",
    "    \"\"\"\n",
    "    Creates a dataframe from a text file\n",
    "    param: name_file: name of the text file, start_time: beginning of the desired period, end_time: end of the desired period\n",
    "    \"\"\"\n",
    "    df_topic = create_dataframe(name_file)\n",
    "    df_topic['Topics'] = remove_space(df_topic)\n",
    "    df_topic = parse_topics_into_df(df_topic, lan, start_time, end_time)\n",
    "    df_pageviews = scrape_pageviews(df_topic)\n",
    "    df_pageviews.fillna(0, inplace=True)\n",
    "    return df_pageviews, df_topic\n",
    "\n",
    "def create_dataframe_pageviews_v2(df_topic_lan, lan, start_time = '20190101', end_time = '20200731'):\n",
    "    \"\"\"\n",
    "    Creates a dataframe from a text file\n",
    "    param: name_file: name of the text file, start_time: beginning of the desired period, end_time: end of the desired period\n",
    "    \"\"\"\n",
    "    df_topic = pd.DataFrame()\n",
    "    df_topic['Topics'] = df_topic_lan[f'{lan}']\n",
    "    df_topic['Topics'] = remove_space(df_topic)\n",
    "    df_topic = parse_topics_into_df(df_topic, lan, start_time, end_time)\n",
    "    df_pageviews = scrape_pageviews_v2(df_topic)\n",
    "    df_pageviews.fillna(0, inplace=True)\n",
    "    df_topic_lan[f'{lan}'] = df_topic['Topics']\n",
    "    return df_pageviews, df_topic_lan\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get translated topics found on Wikidata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_in_language(english_label, target_language):\n",
    "    # Endpoint URL for the Wikidata Query Service\n",
    "    url = \"https://query.wikidata.org/sparql\"\n",
    "    \n",
    "    # SPARQL query to get the item with the English label and its label in the target language\n",
    "    query = f'''\n",
    "    SELECT ?item ?itemLabel WHERE {{\n",
    "      ?item rdfs:label \"{english_label}\"@en.\n",
    "      SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],{target_language}\". }}\n",
    "    }}\n",
    "    LIMIT 1\n",
    "    '''\n",
    "    \n",
    "    # Headers for the request\n",
    "    headers = {\n",
    "        'User-Agent': 'MyBot/0.1 (myemail@example.com)',\n",
    "        'Accept': 'application/sparql-results+json'\n",
    "    }\n",
    "    \n",
    "    # Make the GET request\n",
    "    response = requests.get(url, headers=headers, params={'query': query, 'format': 'json'})\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        results = data['results']['bindings']\n",
    "        if results:\n",
    "            # Return the item and its label in the target language\n",
    "            return results[0]['itemLabel']['value']\n",
    "        else:\n",
    "            return '_' # No label found for this language\n",
    "    else:\n",
    "        # Handle unsuccessful requests\n",
    "        response.raise_for_status()\n",
    "\n",
    "def get_label_in_english(label, source_language):\n",
    "    # Endpoint URL for the Wikidata Query Service\n",
    "    url = \"https://query.wikidata.org/sparql\"\n",
    "\n",
    "    # SPARQL query to get the item with the label in the source language and its English label\n",
    "    query = f'''\n",
    "    SELECT ?item ?itemLabel WHERE {{\n",
    "      ?item rdfs:label \"{label}\"@{source_language}.\n",
    "      SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }}\n",
    "    }}\n",
    "    LIMIT 1\n",
    "    '''\n",
    "\n",
    "    # Headers for the request\n",
    "    headers = {\n",
    "        'User-Agent': 'MyBot/0.1 (myemail@example.com)',\n",
    "        'Accept': 'application/sparql-results+json'\n",
    "    }\n",
    "\n",
    "    # Make the GET request\n",
    "    response = requests.get(url, headers=headers, params={'query': query, 'format': 'json'})\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        results = data['results']['bindings']\n",
    "        if results:\n",
    "            # Return the item and its English label\n",
    "            return results[0]['itemLabel']['value']\n",
    "        else:\n",
    "            return '_'  # No label found for this language\n",
    "    else:\n",
    "        # Handle unsuccessful requests\n",
    "        response.raise_for_status()\n",
    "    \n",
    "print(get_label_in_english('estetica', 'it'))\n",
    "\n",
    "def change_Q(name):\n",
    "    if name.startswith('Q'):\n",
    "        return '_'\n",
    "    else:\n",
    "        return name\n",
    "\n",
    "def capitalize_first_letter(value):\n",
    "    string = str(value)\n",
    "    return string[0].upper() + string[1:].lower() if string else ''\n",
    "\n",
    "def translate_topics(df_topic, lang):\n",
    "    # Run through all the topics and get the translation in Italien and store it in a new column in the DataFrame\n",
    "    df_topic['Topics'] = df_topic['Topics'].str.lower()\n",
    "    df_topic[f'{lang}'] = df_topic['Topics'].apply(lambda x: get_label_in_language(x, lang))\n",
    "    df_topic[f'{lang}'] = df_topic[f'{lang}'].apply(lambda x: change_Q(x))\n",
    "    df_topic[f'{lang}'] = df_topic[f'{lang}'].replace('_', np.nan)\n",
    "    df_topics = pd.DataFrame()\n",
    "    df_topics = df_topic[f'{lang}']\n",
    "    df_topics = df_topics.dropna()\n",
    "    df_topics = df_topics.apply(lambda x: capitalize_first_letter(x))\n",
    "    return df_topics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get pageviews from wikidata for specific country (still not working)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pageviews_for_country(language, project, country, topic, start_date, end_date):\n",
    "\n",
    "    base_url = \"https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article\"\n",
    "    \n",
    "    # If a country is specified, include it in the endpoint\n",
    "    if country:\n",
    "        endpoint = f\"{language}.{project}.org/all-access/all-agents/{topic}/\"\n",
    "    else:\n",
    "        endpoint = f\"{language}.{project}.org/all-access/all-agents\"\n",
    "\n",
    "    # Construct the API URL\n",
    "    url = f\"{base_url}/{endpoint}\"\n",
    "\n",
    "    headers = {\n",
    "        'User-Agent': 'MyBot/0.1 (myemail@example.com)',\n",
    "        'Accept': 'application/json'\n",
    "    }\n",
    "\n",
    "    # Set parameters\n",
    "    params = {\n",
    "        \"start\": start_date,\n",
    "        \"end\": end_date,\n",
    "        \"access\": \"all-access\",\n",
    "        \"agent\": \"all-agents\",\n",
    "        \"granularity\": \"daily\",\n",
    "        \"geotarget\": country,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Make the API request\n",
    "        response = requests.get(url, params=params, headers=headers)\n",
    "        response.raise_for_status()  # Raise an HTTPError for bad responses (4xx and 5xx)\n",
    "\n",
    "        # Check if the response is not empty\n",
    "        if response.text:\n",
    "            data = response.json()\n",
    "            df = pd.DataFrame(data[\"items\"])\n",
    "            return df\n",
    "        else:\n",
    "            print(\"Empty response.\")\n",
    "            return pd.DataFrame()  # Return an empty DataFrame in case of an empty response\n",
    "\n",
    "    except JSONDecodeError as e:\n",
    "        print(f\"JSON Decode Error: {e}\")\n",
    "        return pd.DataFrame()  # Return an empty DataFrame in case of a JSONDecodeError\n",
    "    except RequestException as e:\n",
    "        print(f\"Request Exception: {e}\")\n",
    "        print(f\"URL: {url}\")  # Print the URL for debugging\n",
    "        return pd.DataFrame()  # Return an empty DataFrame in case of a RequestException\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using google trends API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Trends(topic, country):\n",
    "    # Create a pytrends object\n",
    "    pytrends = TrendReq(hl='en-US', tz=10)\n",
    "\n",
    "    # Build payload with country geotarget\n",
    "    kw_list = [topic]\n",
    "    pytrends.build_payload(kw_list, cat=0, timeframe='2019-01-01 2020-07-31', geo=country, gprop='')\n",
    "\n",
    "    # Get interest over time\n",
    "    try:\n",
    "        interest_over_time_df = pytrends.interest_over_time()\n",
    "        return interest_over_time_df\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get a .csv file containg all languages for a specific topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gen_df_topics(name_file, csv_name):\n",
    "    '''\n",
    "    'it': 'Italian','cs': 'Czech','ro': 'Romanian','sv': 'Swedish','fi': 'Finnish','da': 'Danish'\n",
    "    '''\n",
    "\n",
    "    df_pageviews, df_topic = create_dataframe_pageviews(name_file, 'en', '20190101', '20200731')\n",
    "\n",
    "    df_topic_1 = translate_topics(df_topic, 'it')\n",
    "    df_topic_2 = translate_topics(df_topic, 'cs')\n",
    "    df_topic_3 = translate_topics(df_topic, 'ro')\n",
    "    df_topic_4 = translate_topics(df_topic, 'sv')\n",
    "    df_topic_5 = translate_topics(df_topic, 'fi')\n",
    "    df_topic_6 = translate_topics(df_topic, 'da')\n",
    "\n",
    "    df = pd.concat([df_topic_1, df_topic_2, df_topic_3, df_topic_4, df_topic_5, df_topic_6], axis=1, join='inner')\n",
    "\n",
    "    df_topic_o = df_topic['Topics']\n",
    "    df_gen = pd.concat([df_topic_o, df], axis=1, join='inner')\n",
    "    df_gen = df_gen.reset_index(drop=True)\n",
    "\n",
    "    return df_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gen_df_pageviews(df, name_file):\n",
    "    '''\n",
    "    'it': 'Italian','cs': 'Czech','ro': 'Romanian','sv': 'Swedish','fi': 'Finnish','da': 'Danish'\n",
    "    '''\n",
    "    lan = ['it', 'cs', 'ro', 'sv', 'fi', 'da']\n",
    "    for i in lan:\n",
    "        df_i = df[i]\n",
    "        df_i = df_i.to_frame()\n",
    "        df_pageviews, df_topic = create_dataframe_pageviews_v2(df_i, i, '20190101', '20200731')\n",
    "        df_pageviews = get_mean_pageviews(df_pageviews)\n",
    "        df_pageviews.to_csv(f'Wiki-pageviews/{name_file}/{name_file}_pageviews_{i}.csv', index=False)\n",
    "        print(f'{i} done and its pageviews shape is {df_pageviews.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get pageviews for each topic in different languages \n",
    "- Translate topics to each language : \n",
    "it: Italian, cs: Czech, ro: Romanian, sv: Swedish, fi: Finnish, da: Danish, sq: Albanian\n",
    "- Store only topics that have a valid translation in every language\n",
    "- Get pageviews of these topics in all selected languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pageviews_all_lang(name_file, csv_name):\n",
    "    df_gen = get_gen_df_topics(name_file, csv_name)\n",
    "    get_gen_df_pageviews(df_gen, csv_name)\n",
    "    df_gen['Topics'] = df_gen['Topics'].apply(lambda x: x.replace('_', ' ').replace(\"'\", \"\")).apply(lambda x: x[0].upper() + x[1:].lower() if x else '')\n",
    "    df_gen['Topics'].to_csv(f'Wiki-pageviews/{csv_name}/{csv_name}_topics.txt', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it done and its pageviews shape is (19, 50)\n",
      "cs done and its pageviews shape is (19, 50)\n",
      "ro done and its pageviews shape is (19, 50)\n",
      "sv done and its pageviews shape is (19, 50)\n",
      "fi done and its pageviews shape is (19, 50)\n",
      "da done and its pageviews shape is (19, 50)\n"
     ]
    }
   ],
   "source": [
    "get_pageviews_all_lang('Wiki-pageviews/esteem/esteem_topics.txt', 'esteem')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it done and its pageviews shape is (19, 50)\n",
      "cs done and its pageviews shape is (19, 50)\n",
      "ro done and its pageviews shape is (19, 50)\n",
      "sv done and its pageviews shape is (19, 50)\n",
      "fi done and its pageviews shape is (19, 50)\n",
      "da done and its pageviews shape is (19, 50)\n"
     ]
    }
   ],
   "source": [
    "get_pageviews_all_lang('Wiki-pageviews/self_actualization/self_actualization_topics.txt', 'self_actualization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it done and its pageviews shape is (19, 50)\n",
      "cs done and its pageviews shape is (19, 50)\n",
      "ro done and its pageviews shape is (19, 50)\n",
      "sv done and its pageviews shape is (19, 50)\n",
      "fi done and its pageviews shape is (19, 50)\n",
      "da done and its pageviews shape is (19, 50)\n"
     ]
    }
   ],
   "source": [
    "get_pageviews_all_lang('Wiki-pageviews/love_belonging/love_belonging_topics.txt', 'love_belonging')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_i = df[i]\n",
    "df_i = df_i.to_frame()\n",
    "df_pageviews, df_topic = create_dataframe_pageviews_v2(df_i, i, '20190101', '20200731')\n",
    "df_pageviews = get_mean_pageviews(df_pageviews)\n",
    "df_pageviews.to_csv(f'Wiki-pageviews/{name_file}/{name_file}_pageviews_{i}.csv', index=False)\n",
    "print(f'{i} done and its pageviews shape is {df_pageviews.shape}')\n",
    "\n",
    "df_gen['Topics'] = df_gen['Topics'].apply(lambda x: x.replace('_', ' ').replace(\"'\", \"\")).apply(lambda x: x[0].upper() + x[1:].lower() if x else '')\n",
    "df_gen['Topics'].to_csv(f'Wiki-pageviews/{csv_name}/{csv_name}_topics.txt', header=False, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
