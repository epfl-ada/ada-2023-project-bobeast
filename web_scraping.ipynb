{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Maslow's_Hierarchy_of_Needs.svg.png\" height = 600px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import urllib.parse # combine URL components into URL string\n",
    "import wikipediaapi # query wikipedia through api\n",
    "\n",
    "from pytrends.request import TrendReq # Google Trends API\n",
    "\n",
    "from statsmodels.tsa.seasonal import STL # seasonal decompositions\n",
    "import statsmodels.tsa.stattools as smt\n",
    "\n",
    "\n",
    "import pickle #  to serialize and deserialize objects in Python\n",
    "from requests.exceptions import RequestException\n",
    "import requests\n",
    "from json.decoder import JSONDecodeError\n",
    "from scipy import signal\n",
    "import warnings\n",
    "from urllib import request\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import pytrends\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import wikipediaapi\n",
    "import pprint as pp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a dataframe of pageviews per topic from a .txt using Wikipedia API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe(name_file):\n",
    "    \"\"\"\n",
    "    Creates a dataframe from a text file\n",
    "    param: name_file: name of the text file\n",
    "    return: dataframe with the text file\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(name_file, delimiter=\"\\t\", header=None, names=['Topics'])\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    return df\n",
    "\n",
    "# Take only starting from the second word in each row\n",
    "def remove_space(df):\n",
    "    return df['Topics'].apply(lambda x: x.strip().replace(' ', '_'))\n",
    "\n",
    "# Parse the topics into the URL format\n",
    "def parse_topics_into_df(df, start_time, end_time):\n",
    "    # change the spaces to underscores\n",
    "    df['url'] = np.zeros(len(df))\n",
    "    for index, row in df.iterrows():\n",
    "        topic_value = row['Topics']\n",
    "        df.loc[index, 'url'] = f'https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/en.wikipedia.org/all-access/all-agents/{topic_value}/monthly/{start_time}/{end_time}'\n",
    "\n",
    "    return df\n",
    "\n",
    "# Create a new dataframe with timestamp from starting date to ending date\n",
    "def create_dataframe_timestamp(starting_date, ending_date):\n",
    "    df_timestamp = pd.DataFrame()\n",
    "    df_timestamp['Timestamp'] = pd.date_range(start=starting_date, end=ending_date, freq='MS')\n",
    "    return df_timestamp\n",
    "\n",
    "# Define a function to fetch data from the URL and handle errors\n",
    "def fetch_and_parse_url(url):\n",
    "    try:\n",
    "        request.urlopen(url).read()\n",
    "        return True\n",
    "    except request.HTTPError as e:\n",
    "        if e.code == 404:\n",
    "            return None  # or any other value or action you prefer for 404 errors\n",
    "        else:\n",
    "            return None  # or handle other HTTP errors as needed\n",
    "    except Exception as e:\n",
    "        \n",
    "        return None  # or handle other exceptions as needed\n",
    "\n",
    "def get_pageviews_wiki(url):\n",
    "    \"\"\"\n",
    "    Gets the weekly pageviews for one Wikipedia page in one language in the desired period\n",
    "    param: url: url of the Wikipedia page\n",
    "    param: start_date: beginning of the desired period \n",
    "    param: end_date: end of the desired period \n",
    "    return: dataframe column with the monthly pageviews\n",
    "    \"\"\"\n",
    "    html = request.urlopen(url).read()\n",
    "    soup = BeautifulSoup(html,'html.parser')\n",
    "    site_json=json.loads(soup.text)\n",
    "    df=pd.DataFrame(site_json['items'])\n",
    "    df=df['views']\n",
    "    return df\n",
    "\n",
    "def scrape_pageviews(df):\n",
    "    pageview = pd.DataFrame()\n",
    "    pageview['Timestamp'] = create_dataframe_timestamp('2019-01-01', '2020-07-31')['Timestamp']\n",
    "\n",
    "    # Loop through the rows of the DataFrame and append the results of the function to the DataFrame\n",
    "    for index, row in df.iterrows():\n",
    "        url = row['url']\n",
    "        if fetch_and_parse_url(url):\n",
    "            pageview_solo = pd.DataFrame()\n",
    "            pageview_solo[row['Topics']] = get_pageviews_wiki(url)\n",
    "            pageview = pd.concat([pageview, pageview_solo], axis=1)\n",
    "\n",
    "    return pageview\n",
    "\n",
    "\n",
    "def create_dataframe_pageviews(name_file, start_time = '20190101', end_time = '20200731'):\n",
    "    \"\"\"\n",
    "    Creates a dataframe from a text file\n",
    "    param: name_file: name of the text file, start_time: beginning of the desired period, end_time: end of the desired period\n",
    "    \"\"\"\n",
    "    df_topic = create_dataframe(name_file)\n",
    "    df_topic['Topics'] = remove_space(df_topic)\n",
    "    df_topic = parse_topics_into_df(df_topic, start_time, end_time)\n",
    "    df_pageviews = scrape_pageviews(df_topic)\n",
    "    df_pageviews.fillna(0, inplace=True)\n",
    "    return df_pageviews, df_topic\n",
    "\n",
    "def list_valid_topics(df_pageviews):\n",
    "    lista = df_pageviews.columns.tolist()\n",
    "    return lista[1:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get translated topics found on Wikidata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_in_language(english_label, target_language):\n",
    "    # Endpoint URL for the Wikidata Query Service\n",
    "    url = \"https://query.wikidata.org/sparql\"\n",
    "    \n",
    "    # SPARQL query to get the item with the English label and its label in the target language\n",
    "    query = f'''\n",
    "    SELECT ?item ?itemLabel WHERE {{\n",
    "      ?item rdfs:label \"{english_label}\"@en.\n",
    "      SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],{target_language}\". }}\n",
    "    }}\n",
    "    LIMIT 1\n",
    "    '''\n",
    "    \n",
    "    # Headers for the request\n",
    "    headers = {\n",
    "        'User-Agent': 'MyBot/0.1 (myemail@example.com)',\n",
    "        'Accept': 'application/sparql-results+json'\n",
    "    }\n",
    "    \n",
    "    # Make the GET request\n",
    "    response = requests.get(url, headers=headers, params={'query': query, 'format': 'json'})\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        results = data['results']['bindings']\n",
    "        if results:\n",
    "            # Return the item and its label in the target language\n",
    "            return results[0]['itemLabel']['value']\n",
    "        else:\n",
    "            return '_' # No label found for this language\n",
    "    else:\n",
    "        # Handle unsuccessful requests\n",
    "        response.raise_for_status()\n",
    "\n",
    "def change_Q(name):\n",
    "    if name.startswith('Q'):\n",
    "        return ' '\n",
    "    else:\n",
    "        return name\n",
    "\n",
    "def translate_topics(df_topic, lang):\n",
    "    # Run through all the topics and get the translation in Italien and store it in a new column in the DataFrame\n",
    "    df_topic['Topics'] = df_topic['Topics'].str.lower()\n",
    "    df_topic[f'{lang}'] = df_topic['Topics'].apply(lambda x: get_label_in_language(x, lang))\n",
    "    df_topic[f'{lang}'].replace('No label found for this language', ' ', inplace=True)\n",
    "    df_topic[f'{lang}'] = df_topic[f'{lang}'].apply(lambda x: change_Q(x))\n",
    "    return df_topic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get pageviews from wikidata for specific country (still not working)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request Exception: 404 Client Error: Not Found for url: https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/en.wikipedia.org/all-access/all-agents/Covid/?start=2022010100&end=2022013100&access=all-access&agent=all-agents&granularity=daily&geotarget=US\n",
      "URL: https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/en.wikipedia.org/all-access/all-agents/Covid/\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "def get_pageviews_for_country(language, project, country, topic, start_date, end_date):\n",
    "\n",
    "    base_url = \"https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article\"\n",
    "    \n",
    "    # If a country is specified, include it in the endpoint\n",
    "    if country:\n",
    "        endpoint = f\"{language}.{project}.org/all-access/all-agents/{topic}/\"\n",
    "    else:\n",
    "        endpoint = f\"{language}.{project}.org/all-access/all-agents\"\n",
    "\n",
    "    # Construct the API URL\n",
    "    url = f\"{base_url}/{endpoint}\"\n",
    "\n",
    "    headers = {\n",
    "        'User-Agent': 'MyBot/0.1 (myemail@example.com)',\n",
    "        'Accept': 'application/json'\n",
    "    }\n",
    "\n",
    "    # Set parameters\n",
    "    params = {\n",
    "        \"start\": start_date,\n",
    "        \"end\": end_date,\n",
    "        \"access\": \"all-access\",\n",
    "        \"agent\": \"all-agents\",\n",
    "        \"granularity\": \"daily\",\n",
    "        \"geotarget\": country,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Make the API request\n",
    "        response = requests.get(url, params=params, headers=headers)\n",
    "        response.raise_for_status()  # Raise an HTTPError for bad responses (4xx and 5xx)\n",
    "\n",
    "        # Check if the response is not empty\n",
    "        if response.text:\n",
    "            data = response.json()\n",
    "            df = pd.DataFrame(data[\"items\"])\n",
    "            return df\n",
    "        else:\n",
    "            print(\"Empty response.\")\n",
    "            return pd.DataFrame()  # Return an empty DataFrame in case of an empty response\n",
    "\n",
    "    except JSONDecodeError as e:\n",
    "        print(f\"JSON Decode Error: {e}\")\n",
    "        return pd.DataFrame()  # Return an empty DataFrame in case of a JSONDecodeError\n",
    "    except RequestException as e:\n",
    "        print(f\"Request Exception: {e}\")\n",
    "        print(f\"URL: {url}\")  # Print the URL for debugging\n",
    "        return pd.DataFrame()  # Return an empty DataFrame in case of a RequestException\n",
    "\n",
    "# Example usage\n",
    "language_code = \"en\"\n",
    "project_code = \"wikipedia\"\n",
    "country_code = \"US\"\n",
    "start_date = \"2022010100\"\n",
    "end_date = \"2022013100\"\n",
    "topic = \"Covid\"\n",
    "\n",
    "result = get_pageviews_for_country(language_code, project_code, country_code, topic, start_date, end_date)\n",
    "\n",
    "# Output the result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using google trends API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Trends(topic, country):\n",
    "    # Create a pytrends object\n",
    "    pytrends = TrendReq(hl='en-US', tz=10)\n",
    "\n",
    "    # Build payload with country geotarget\n",
    "    kw_list = [topic]\n",
    "    pytrends.build_payload(kw_list, cat=0, timeframe='2019-01-01 2020-07-31', geo=country, gprop='')\n",
    "\n",
    "    # Get interest over time\n",
    "    try:\n",
    "        interest_over_time_df = pytrends.interest_over_time()\n",
    "        return interest_over_time_df\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List all valid topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Abstract_Art', 'Creative_Writing', 'Musical_Composition', 'Graphic_Novels', 'Experimental_Music', 'Abstract_Realism', 'Creative_Industries', 'Creative_Arts', 'Creative_Visualization', 'Creative_Education', 'Creative_Technology', 'Creative_Economy', 'Innovation', 'Imagination', 'Artistry', 'Design', 'Inspiration', 'Expression', 'Ingenuity', 'Originality', 'Inventiveness', 'Creation', 'Vision', 'Talent', 'Craftsmanship', 'Ideation', 'Aesthetics', 'Productivity', 'Composition', 'Insight', 'Motivation', 'Pioneering', 'Resourcefulness', 'Genius', 'Creativity', 'Novelty', 'Mastery', 'Conceptualization', 'Futurism', 'Breakthrough', 'Divergence', 'Artisan', 'Epiphany', 'Experimentation', 'Productiveness', 'Conceptualize', 'Original', 'Renewal', 'Formulation', 'Symbiosis', 'Trailblazing', 'Uniqueness', 'Redefinition', 'Visionary', 'Adaptability', 'Illumination', 'Progression', 'Renaissance', 'Catalyst', 'Transformation', 'Vitality', 'Modernism', 'Contemporary', 'Imaginative', 'Flourishing', 'Evolution', 'Exploration', 'Conceptual', 'Progress', 'Experiment', 'Inventive', 'Intuition', 'Sculpture', 'Integrate', 'Synthesize', 'Momentum', 'Vibrancy', 'Illuminate', 'Reform', 'Rejuvenate', 'Aspiration', 'Resilience', 'Envision', 'Pioneer', 'Unveil', 'Modernize', 'Infuse', 'Inspire', 'Transform', 'Invent', 'Imagine', 'Innovate', 'Create', 'Improvisation', 'Design_Thinking', 'Critical_Thinking', 'Problem_Solving', 'Brainstorming', 'Creative_Problem_Solving', 'Divergent_Thinking', 'Tolerance', 'Open-mindedness', 'Diversity', 'Equity', 'Inclusivity', 'Acceptance', 'Pluralism', 'Fairness', 'Equal_Opportunity', 'Human_Rights', 'Unbiased', 'Neutrality', 'Equality', 'Empathy', 'Impartiality', 'Anti-discrimination', 'Factual_Accuracy', 'Anti-racism', 'Non-discrimination', 'Objectivity', 'Social_Equality', 'Non-partisanship', 'Racial_Equality', 'Non-discriminatory', 'Non-biased', 'Non-discriminating', 'Anti-stereotype']\n",
      "['Abstract Art', 'Creative Writing', 'Musical Composition', 'Graphic Novels', 'Experimental Music', 'Abstract Realism', 'Creative Industries', 'Creative Arts', 'Creative Visualization', 'Creative Education', 'Creative Technology', 'Creative Economy', 'Innovation', 'Imagination', 'Artistry', 'Design', 'Inspiration', 'Expression', 'Ingenuity', 'Originality', 'Inventiveness', 'Creation', 'Vision', 'Talent', 'Craftsmanship', 'Ideation', 'Aesthetics', 'Productivity', 'Composition', 'Insight', 'Motivation', 'Pioneering', 'Resourcefulness', 'Genius', 'Creativity', 'Novelty', 'Mastery', 'Conceptualization', 'Futurism', 'Breakthrough', 'Divergence', 'Artisan', 'Epiphany', 'Experimentation', 'Productiveness', 'Conceptualize', 'Original', 'Renewal', 'Formulation', 'Symbiosis', 'Trailblazing', 'Uniqueness', 'Redefinition', 'Visionary', 'Adaptability', 'Illumination', 'Progression', 'Renaissance', 'Catalyst', 'Transformation', 'Vitality', 'Modernism', 'Contemporary', 'Imaginative', 'Flourishing', 'Evolution', 'Exploration', 'Conceptual', 'Progress', 'Experiment', 'Inventive', 'Intuition', 'Sculpture', 'Integrate', 'Synthesize', 'Momentum', 'Vibrancy', 'Illuminate', 'Reform', 'Rejuvenate', 'Aspiration', 'Resilience', 'Envision', 'Pioneer', 'Unveil', 'Modernize', 'Infuse', 'Inspire', 'Transform', 'Invent', 'Imagine', 'Innovate', 'Create', 'Improvisation', 'Design Thinking', 'Critical Thinking', 'Problem Solving', 'Brainstorming', 'Creative Problem Solving', 'Divergent Thinking', 'Tolerance', 'Open-mindedness', 'Diversity', 'Equity', 'Inclusivity', 'Acceptance', 'Pluralism', 'Fairness', 'Equal Opportunity', 'Human Rights', 'Unbiased', 'Neutrality', 'Equality', 'Empathy', 'Impartiality', 'Anti-discrimination', 'Factual Accuracy', 'Anti-racism', 'Non-discrimination', 'Objectivity', 'Social Equality', 'Non-partisanship', 'Racial Equality', 'Non-discriminatory', 'Non-biased', 'Non-discriminating', 'Anti-stereotype']\n"
     ]
    }
   ],
   "source": [
    "lista = list_valid_topics(df_pageviews)\n",
    "print(lista)\n",
    "lista = [s.replace('_', ' ').replace(\"'\", \"\") for s in lista]\n",
    "print(lista)\n",
    "\n",
    "# Add to .txt file this list\n",
    "with open('Self_actualization/self_actualization1.txt', 'w') as f:\n",
    "    for item in lista:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
